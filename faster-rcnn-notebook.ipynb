{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rajiv\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rajiv\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rajiv\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp310-cp310-win_amd64.whl (2728.9 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp310-cp310-win_amd64.whl (5.3 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.13.1 fsspec-2024.6.1 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 pillow-11.0.0 sympy-1.13.1 torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.8-cp310-cp310-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rajiv\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from pycocotools) (2.1.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.57.0-cp310-cp310-win_amd64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rajiv\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\rajiv\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rajiv\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rajiv\\anaconda3\\envs\\yoloenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached pycocotools-2.0.8-cp310-cp310-win_amd64.whl (84 kB)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "Using cached matplotlib-3.10.1-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.57.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, opencv-python, kiwisolver, fonttools, cycler, contourpy, matplotlib, pycocotools\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 opencv-python-4.11.0.86 pycocotools-2.0.8 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))': /simple/pycocotools/\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install pycocotools opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch [1/20] - Total Loss: 44.9601\n",
      "Epoch [2/20] - Total Loss: 28.5018\n",
      "Epoch [3/20] - Total Loss: 23.4342\n",
      "Epoch [4/20] - Total Loss: 21.2129\n",
      "Epoch [5/20] - Total Loss: 19.9704\n",
      "Epoch [6/20] - Total Loss: 18.1485\n",
      "Epoch [7/20] - Total Loss: 17.0658\n",
      "Epoch [8/20] - Total Loss: 16.8957\n",
      "Epoch [9/20] - Total Loss: 15.0604\n",
      "Epoch [10/20] - Total Loss: 14.3655\n",
      "Epoch [11/20] - Total Loss: 13.5928\n",
      "Epoch [12/20] - Total Loss: 12.9297\n",
      "Epoch [13/20] - Total Loss: 12.7256\n",
      "Epoch [14/20] - Total Loss: 12.4759\n",
      "Epoch [15/20] - Total Loss: 11.6398\n",
      "Epoch [16/20] - Total Loss: 11.3505\n",
      "Epoch [17/20] - Total Loss: 11.6008\n",
      "Epoch [18/20] - Total Loss: 10.7517\n",
      "Epoch [19/20] - Total Loss: 10.4242\n",
      "Epoch [20/20] - Total Loss: 9.8146\n"
     ]
    }
   ],
   "source": [
    "# All imports are handled here\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# We set up the dataset for Faster RCNN here\n",
    "\n",
    "class CocoDetectionForFasterRCNN(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, targets = super().__getitem__(idx)\n",
    "        img_id = self.ids[idx]\n",
    "        anno = [obj for obj in targets if obj.get(\"iscrowd\", 0) == 0]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in anno:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(obj[\"category_id\"])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([img_id])\n",
    "        }\n",
    "\n",
    "        if self._transforms:\n",
    "            img = self._transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# All relevant paths are listed here\n",
    "\n",
    "DATASET_PATH = \"dataset_coco\"\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"train\")\n",
    "VAL_PATH = os.path.join(DATASET_PATH, \"val\")\n",
    "TRAIN_ANN = os.path.join(DATASET_PATH, \"annotations\", \"instances_train.json\")\n",
    "VAL_ANN = os.path.join(DATASET_PATH, \"annotations\", \"instances_val.json\")\n",
    "NUM_CLASSES = 3  # Only classes and laptops, but an unused class was also accidentally included\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Build the dataset and the DataLoader\n",
    "\n",
    "train_dataset = CocoDetectionForFasterRCNN(TRAIN_PATH, TRAIN_ANN, transforms=F.to_tensor)\n",
    "val_dataset = CocoDetectionForFasterRCNN(VAL_PATH, VAL_ANN, transforms=F.to_tensor)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load the Faster RCNN ResNet 50 model\n",
    "\n",
    "from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "# Train the model here\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "num_epochs = 20\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Total Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to fasterrcnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"fasterrcnn_model.pth\")\n",
    "print(\"✅ Model saved to fasterrcnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running inference on test set...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.756\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.908\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.908\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.689\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.760\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.630\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.812\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.812\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.713\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.819\n",
      "\n",
      "mAP@0.5:         0.9078\n",
      "mAP@0.5:0.95:    0.7562\n",
      "Inference Speed: 89.03 ms/image\n",
      "Model File Size: 158.07 MB\n",
      "Trainable Params: 41.08 million\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# All paths to data and models go here\n",
    "\n",
    "TEST_IMG_DIR = \"dataset_coco/test\"\n",
    "TEST_ANN_FILE = \"dataset_coco/annotations/instances_test.json\"\n",
    "MODEL_PATH = \"fasterrcnn_model.pth\"\n",
    "NUM_CLASSES = 3  # Includes unused class\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom wrapper for the COCO dataset\n",
    "\n",
    "class CocoDetectionForFRCNN(CocoDetection):\n",
    "    def __getitem__(self, idx):\n",
    "        img, targets = super().__getitem__(idx)\n",
    "        img_id = self.ids[idx]\n",
    "        anno = [obj for obj in targets if obj.get(\"iscrowd\", 0) == 0]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in anno:\n",
    "            x, y, w, h = obj[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(obj[\"category_id\"])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([img_id])\n",
    "        }\n",
    "        return F.to_tensor(img), target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_model_size(model_path):\n",
    "    size_bytes = os.path.getsize(model_path)\n",
    "    return round(size_bytes / (1024 * 1024), 2)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Reload the model here\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=None)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the test set here\n",
    "\n",
    "test_dataset = CocoDetectionForFRCNN(TEST_IMG_DIR, TEST_ANN_FILE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn)\n",
    "\n",
    "# Code for inference and MAP evaluation\n",
    "\n",
    "print(\"Running inference on test set...\")\n",
    "predictions = []\n",
    "start = time.time()\n",
    "\n",
    "for imgs, targets in test_loader:\n",
    "    imgs = [img.to(device) for img in imgs]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(imgs)\n",
    "\n",
    "    for output, target in zip(outputs, targets):\n",
    "        boxes = output['boxes'].cpu().tolist()\n",
    "        scores = output['scores'].cpu().tolist()\n",
    "        labels = output['labels'].cpu().tolist()\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            predictions.append({\n",
    "                \"image_id\": int(target['image_id']),\n",
    "                \"category_id\": int(label),\n",
    "                \"bbox\": [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "\n",
    "total_time = time.time() - start\n",
    "avg_time = total_time / len(test_dataset)\n",
    "\n",
    "# Save predictions and compute the COCO map\n",
    "\n",
    "with open(\"frcnn_test_predictions.json\", \"w\") as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "coco_gt = COCO(TEST_ANN_FILE)\n",
    "coco_dt = coco_gt.loadRes(\"frcnn_test_predictions.json\")\n",
    "\n",
    "evaluator = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "evaluator.evaluate()\n",
    "evaluator.accumulate()\n",
    "evaluator.summarize()\n",
    "\n",
    "# Evaluate all results here\n",
    "\n",
    "mAP_50_95 = evaluator.stats[0]\n",
    "mAP_50 = evaluator.stats[1]\n",
    "model_file_size = get_model_size(MODEL_PATH)\n",
    "param_count = count_parameters(model) / 1e6  # in millions\n",
    "\n",
    "# Summary\n",
    "\n",
    "print(f\"\\nmAP@0.5:         {mAP_50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95:    {mAP_50_95:.4f}\")\n",
    "print(f\"Inference Speed: {avg_time * 1000:.2f} ms/image\")\n",
    "print(f\"Model File Size: {model_file_size:.2f} MB\")\n",
    "print(f\"Trainable Params: {param_count:.2f} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
